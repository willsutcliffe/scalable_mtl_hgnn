model:
  type: "heterognn"
  gnn_layers: 4
  mlp_output_size: 16  # -> this to 32? This one is the output of the global, edge and node blocks
  mlp_layers: 4
  mlp_channels: 128  # hidden dimension for the same as above
  weight_mlp_layers: 4
  weight_mlp_channels: 16  # Hidden dimension for the MLP in the inference part -> change to 32
  weighted_mp: False
  use_edge_weights: True
  use_node_weights: True
  node_types: ['tracks', 'pvs']
  edge_types: ["tracks_tracks","tracks_pvs"]
  norm: "batch_norm"
  dropout: 0.01
  # Some FT flags
  nFT_layers: 1  # starting from the end
  no_FT_epochs: 2 # counting start at 0 -> 3 epochs w/ FT disabled

# Now additional information for training++
data_dir: "/eos/user/y/yukaiz/DFEI_data"
mode: ""  # will be set to either eval or training TODO


training:
  sample: ["inclusive"]
  batch_size: 12
  gacc: 1  # gradient accumulation if using smaller batch size
  ngpu: 1  # need roughly 40 Gb for bs of 12 
  ncpu: 8  # need roughly 128Gb cpu memory
  cpt: None  # Add a checkpoint to continue training or optimizing
  cw: False # Flag if weights need to be calculated or not
  # Figure seomthing for SWA

eval:
  sample: "Bs_JpsiPhi"
  version: 7  # lightning_logs version number
  cpt: "best-epoch=08-val_combined_loss=0.00.ckpt"
  performance:  # Flags for eval for performance determination
    nodes: True
    edges: True
    PV: True
    B_reco: True
    frag: True
  pruning:
    layer: 3  # Layer where the pruning is applied to
    nodes: 0.01
    edges: 0.01
  


