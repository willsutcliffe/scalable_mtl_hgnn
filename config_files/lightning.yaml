model:
  type: "heterognn"
  gnn_layers: 4
  mlp_output_size: 16
  mlp_layers: 4
  mlp_channels: 128
  weight_mlp_layers: 4
  weight_mlp_channels: 16
  weighted_mp: False
  use_edge_weights: True
  use_node_weights: True
  node_types: ['tracks', 'pvs']
  edge_types: ["tracks_tracks","tracks_pvs"]
  norm: "batch_norm"
  dropout: 0.3

# Now additional information for training++
data_dir: "/eos/user/y/yukaiz/DFEI_data"
mode: ""  # will be set to either eval or training


training:
  sample: ["inclusive"]
  batch_size: 12
  gacc: 1  # gradient accumulation if using smaller batch size
  ngpu: 1  # need roughly 40 Gb for bs of 12 
  ncpu: 8  # need roughly 128Gb cpu memory
  cpt: None  # Add a checkpoint to continue training or optimizing
  cw: False # Flag if weights need to be calculated or not
  # Figure seomthing for SWA

eval:
  sample: "Bs_JpsiPhi"
  version: 7  # lightning_logs version number
  cpt: "best-epoch=08-val_combined_loss=0.00.ckpt"
  performance:  # Flags for eval for performance determination
    nodes: True
    edges: True
    PV: True
    B_reco: True
    frag: True
  pruning:
    layer: 3  # Layer where the pruning is applied to
    nodes: 0.2
    edges: 0.2
  


