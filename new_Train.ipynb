{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f6e478-751a-4105-8616-28a531511009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'heterognn', 'gnn_layers': 6, 'mlp_output_size': 16, 'mlp_layers': 4, 'mlp_channels': 128, 'weight_mlp_layers': 4, 'weight_mlp_channels': 16, 'weighted_mp': False, 'use_edge_weights': True, 'use_node_weights': True, 'node_types': ['tracks', 'PVs'], 'edge_types': ['tracks_tracks', 'tracks_PVs']}\n",
      "0.001\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "from wmpgnn.configs.config_loader import ConfigLoader\n",
    "# Load the configuration\n",
    "config_loader = ConfigLoader(\"config_files/heteromp_gnn.yaml\", environment_prefix=\"DL\")\n",
    "print(config_loader.get(\"model\"))          # Outputs: \"resnet50\"\n",
    "print(config_loader.get(\"training.starting_learning_rate\")) # Outputs: 0.001\n",
    "\n",
    "# Override via environment variable\n",
    "#os.environ[\"DL_MODEL_BATCH_SIZE\"] = \"64\"\n",
    "print(config_loader.get(\"training.batch_size\"))    # Outputs: 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7d1823-2522-4213-b689-3bee0a437196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_type': 'heterogeneous', 'data_dir': '/home/sutclw/Work/Zurich/LHCb/GNNs/cached_hetero_data'}\n"
     ]
    }
   ],
   "source": [
    "print(config_loader.get(\"dataset\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706723bd-9f31-4d0c-ab9a-426bb6457291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99978293-8c2f-4976-be64-209a66e343a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773aaf1-48ac-4c9b-a05f-c2b5a661823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9fb8d-9a25-49d5-ba28-31659ac7565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_input_tr = sorted(glob.glob('/home/sutclw/Work/Zurich/LHCb/GNNs/cached_LCA_training_events_PYTHIA/training_dataset/input_*'))\n",
    "# files_target_tr = sorted(glob.glob('/home/sutclw/Work/Zurich/LHCb/GNNs/cached_LCA_training_events_PYTHIA/training_dataset/target_*'))\n",
    "\n",
    "# files_input_vl = sorted(glob.glob('/home/sutclw/Work/Zurich/LHCb/GNNs/cached_LCA_training_events_PYTHIA/validation_dataset/input_*'))\n",
    "# files_target_vl = sorted(glob.glob('/home/sutclw/Work/Zurich/LHCb/GNNs/cached_LCA_training_events_PYTHIA/validation_dataset/target_*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048f48d-652f-4f08-bd14-e3a8950d3cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921881c7-1c4b-4ea4-826c-b8bc256f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# pv_loc = \"/home/sutclw/Work/weighted_MP_gnn/pv_cached_data/\"\n",
    "# for file in files_target_tr:\n",
    "#     # if \"input\" in file:\n",
    "#     #     name = file[file.find(\"input\"):]\n",
    "#     #     cmd= \"cp \" + pv_loc + name + f\" ../cached_hetero_data/validation_dataset/{name}\"\n",
    "#     #     os.system(cmd)\n",
    "#     if \"target\" in file:\n",
    "#         name = file[file.find(\"target\"):]\n",
    "#         cmd= \"cp \" + pv_loc + name + f\" ../cached_hetero_data/training_dataset/{name}\"\n",
    "#         os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b65ff-8ba5-4ab3-ac96-74b681d4352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4419c3e-4188-4e0b-9059-06575212bd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d90ee6-2d26-45f6-bcec-5ec2bfd8fd34",
   "metadata": {},
   "source": [
    "# To do now\n",
    "\n",
    "## Training\n",
    "* load hetero model\n",
    "* save dataframe of results\n",
    "* get hetero dataset in the correct format\n",
    "* check hetero training\n",
    "* Figure out performance loss homog training\n",
    "* integrate options for setting loss pars\n",
    "* Finalize training script\n",
    "\n",
    "## Inference\n",
    "* inference class\n",
    "* reconstruction performance \n",
    "\n",
    "## Paper\n",
    "\n",
    "## Longer term\n",
    "* load transformer model\n",
    "* hyperpar optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e41fc4-5783-4515-8bd1-08bbc6035d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os, time\n",
    "\n",
    "import os.path as osp\n",
    "import glob\n",
    "from wmpgnn.datasets.graph_dataset import CustomDataset\n",
    "from wmpgnn.datasets.hetero_graph_dataset import CustomHeteroDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import contextlib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_scatter import scatter_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b4697e-65e5-452c-b82d-e466a62fd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = config_loader.get(\"dataset.data_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552fee7a-af59-4cea-99b0-f7c3c92ded0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2272ffd-4d37-4541-a275-5756b171c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52aa4953-5a50-4606-a8f2-90c9e9805c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\" Class which loads data using an appropriate \n",
    "        dataset class \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config_loader = config\n",
    "        print\n",
    "        data_path = config_loader.get(\"dataset.data_dir\")\n",
    "        data_type = config_loader.get(\"dataset.data_type\")\n",
    "        files_input_tr = sorted(glob.glob(f'{data_path}/training_dataset/input_*'))\n",
    "        files_target_tr = sorted(glob.glob(f'{data_path}/training_dataset/target_*'))\n",
    "        files_input_vl = sorted(glob.glob(f'{data_path}/validation_dataset/input_*'))\n",
    "        files_target_vl = sorted(glob.glob(f'{data_path}/validation_dataset/target_*'))\n",
    "\n",
    "\n",
    "        if data_type == \"homogeneous\":\n",
    "            self.train_dataset = CustomDataset(files_input_tr, files_target_tr )\n",
    "            self.val_dataset = CustomDataset(files_input_vl, files_target_vl )\n",
    "        elif data_type == \"heterogeneous\":\n",
    "            self.train_dataset = CustomHeteroDataset(files_input_tr, files_target_tr )\n",
    "            self.val_dataset = CustomHeteroDataset(files_input_vl, files_target_vl )  \n",
    "        else:\n",
    "            raise Exception(f\"Unexpected data type {data_type}. Please use homogeneous or heterogeneous.\")\n",
    "            \n",
    "    def load_data(self):\n",
    "         self.dataset_tr = self.train_dataset.get()    \n",
    "         self.dataset_vl = self.val_dataset.get() \n",
    "        \n",
    "    def get_train_dataloader(self, batch_size=32):   \n",
    "        return DataLoader(self.dataset_tr, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "    def get_val_dataloader(self, batch_size=32):    \n",
    "        return DataLoader(self.dataset_vl, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517cd337-e816-4528-820f-0f9863723711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'heterogeneous',\n",
       " 'data_dir': '/home/sutclw/Work/Zurich/LHCb/GNNs/cached_hetero_data'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_loader.get(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341dbb7-e143-4fef-bee5-67cc7b2896e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b9701b-9b29-472d-933a-697a9b580e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataHandler(config_loader)\n",
    "data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c9e2ca2-7aa8-42d1-b021-62a9cbaa5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_loader.get_train_dataloader()\n",
    "val_loader = data_loader.get_val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2e62bb-3da8-444b-a400-c24d58673bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ded4349-24d5-4841-a1a9-9d42854ffeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'heterognn',\n",
       " 'gnn_layers': 6,\n",
       " 'mlp_output_size': 16,\n",
       " 'mlp_layers': 4,\n",
       " 'mlp_channels': 128,\n",
       " 'weight_mlp_layers': 4,\n",
       " 'weight_mlp_channels': 16,\n",
       " 'weighted_mp': False,\n",
       " 'use_edge_weights': True,\n",
       " 'use_node_weights': True,\n",
       " 'node_types': ['tracks', 'PVs'],\n",
       " 'edge_types': ['tracks_tracks', 'tracks_PVs']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_loader = ConfigLoader(\"config_files/heteromp_gnn.yaml\", environment_prefix=\"DL\")\n",
    "\n",
    "config_loader.get(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6116d37b-d2a5-4a52-8541-15fd47da7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edges = config_loader.get(\"model\")['edge_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab7720a-e526-4f99-9d77-ce302fc513a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tracks', 'to', 'tracks'), ('tracks', 'to', 'PVs')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(edge.split('_')[0],'to', edge.split('_')[1]) for edge in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2c10bb3-7098-4b96-9b1c-d226551aed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wmpgnn.model.gnn_model import GNN\n",
    "from wmpgnn.model.hetero_gnn_model import HeteroGNN\n",
    "class ModelLoader:\n",
    "    \"\"\" Class to set up the model \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config_loader = config\n",
    "        model_type = config_loader.get(\"model.type\")\n",
    "        model_type=\"heterognn\"\n",
    "        if model_type == \"mpgnn\":\n",
    "            self.model = GNN(mlp_output_size=config_loader.get(\"model.mlp_output_size\"), edge_op=4, num_blocks=config_loader.get(\"model.gnn_layers\"),\n",
    "                             mlp_layers = config_loader.get(\"model.mlp_layers\"),\n",
    "                             mlp_channels = config_loader.get(\"model.mlp_channels\"),\n",
    "                             weight_mlp_channels = config_loader.get(\"model.weight_mlp_channels\"),\n",
    "                             weight_mlp_layers= config_loader.get(\"model.weight_mlp_layers\"),\n",
    "                             use_edge_weights = config_loader.get(\"model.use_edge_weights\"),\n",
    "                             use_node_weights = config_loader.get(\"model.use_node_weights\"),\n",
    "                             weighted_mp = config_loader.get(\"model.weighted_mp\")\n",
    "                            )                            \n",
    "        elif model_type == \"heterognn\":\n",
    "            nodes = config_loader.get(\"model\")['node_types']\n",
    "            edges = config_loader.get(\"model\")['edge_types']\n",
    "            edges = [(edge.split('_')[0],'to', edge.split('_')[1]) for edge in edges]\n",
    "            self.model = HeteroGNN(node_types = nodes, edge_types = edges,\n",
    "                             mlp_output_size=config_loader.get(\"model.mlp_output_size\"), edge_op=4,\n",
    "                             num_blocks=config_loader.get(\"model.gnn_layers\"),\n",
    "                             mlp_layers = config_loader.get(\"model.mlp_layers\"),\n",
    "                             mlp_channels = config_loader.get(\"model.mlp_channels\"),\n",
    "                             weight_mlp_channels = config_loader.get(\"model.weight_mlp_channels\"),\n",
    "                             weight_mlp_layers= config_loader.get(\"model.weight_mlp_layers\"),\n",
    "                             use_edge_weights = config_loader.get(\"model.use_edge_weights\"),\n",
    "                             use_node_weights = config_loader.get(\"model.use_node_weights\"),\n",
    "                             weighted_mp = config_loader.get(\"model.weighted_mp\")\n",
    "                            )    \n",
    "        elif model_type == \"transformer\":\n",
    "            pass\n",
    "            \n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39af8b-dbb4-4acb-89a6-77c34df20072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0900f-1591-4a3c-b017-8d78f8badd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e629cf0-be84-4c8a-93f7-0577372c3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loader = ModelLoader(config_loader)\n",
    "model = model_loader.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6947250c-d083-4532-a9c3-adf43cf3116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroGNN(\n",
       "  (_encoder): HeteroGraphCoder(\n",
       "    (_global_model): WrappedModelFnModule(\n",
       "      (_model): MLP(-1, 128, 128, 128, 16)\n",
       "    )\n",
       "    (_edge_models_model_dict): ModuleDict(\n",
       "      (('tracks', 'to', 'tracks')): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "      (('tracks', 'to', 'PVs')): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "    )\n",
       "    (_node_models_model_dict): ModuleDict(\n",
       "      (tracks): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "      (PVs): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_blocks): ModuleList(\n",
       "    (0-5): 6 x HeteroGraphNetwork(\n",
       "      (_edge_block): HeteroEdgeBlock(\n",
       "        (_edge_models_model_dict): ModuleDict(\n",
       "          (('tracks', 'to', 'tracks')): MLP(-1, 128, 128, 128, 16)\n",
       "          (('tracks', 'to', 'PVs')): MLP(-1, 128, 128, 128, 16)\n",
       "        )\n",
       "      )\n",
       "      (_node_block): HeteroNodeBlock(\n",
       "        (_received_edges_aggregator): HeteroEdgesToNodesAggregator()\n",
       "        (_sent_edges_aggregator): HeteroEdgesToNodesAggregator()\n",
       "        (_node_models_model_dict): ModuleDict(\n",
       "          (tracks): MLP(-1, 128, 128, 128, 16)\n",
       "          (PVs): MLP(-1, 128, 128, 128, 16)\n",
       "        )\n",
       "      )\n",
       "      (_global_block): HeteroGlobalBlock(\n",
       "        (_global_model): MLP(-1, 128, 128, 128, 16)\n",
       "        (_edges_aggregator): HeteroEdgesToGlobalsAggregator()\n",
       "        (_nodes_aggregator): HeteroNodesToGlobalsAggregator()\n",
       "      )\n",
       "      (_edge_models_model_dict): ModuleDict(\n",
       "        (('tracks', 'to', 'tracks')): MLP(-1, 16, 16, 1)\n",
       "        (('tracks', 'to', 'PVs')): MLP(-1, 16, 16, 1)\n",
       "      )\n",
       "      (_node_models_model_dict): ModuleDict(\n",
       "        (tracks): MLP(-1, 16, 16, 1)\n",
       "        (PVs): MLP(-1, 16, 16, 1)\n",
       "      )\n",
       "      (_sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (_decoder): HeteroGraphCoder(\n",
       "    (_global_model): WrappedModelFnModule(\n",
       "      (_model): MLP(-1, 128, 128, 128, 16)\n",
       "    )\n",
       "    (_edge_models_model_dict): ModuleDict(\n",
       "      (('tracks', 'to', 'tracks')): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "      (('tracks', 'to', 'PVs')): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "    )\n",
       "    (_node_models_model_dict): ModuleDict(\n",
       "      (tracks): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "      (PVs): WrappedModelFnModule(\n",
       "        (_model): MLP(-1, 128, 128, 128, 16)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_output_transform): HeteroGraphCoder(\n",
       "    (_edge_models_model_dict): ModuleDict(\n",
       "      (('tracks', 'to', 'tracks')): WrappedModelFnModule(\n",
       "        (_model): Linear(in_features=16, out_features=4, bias=True)\n",
       "      )\n",
       "      (('tracks', 'to', 'PVs')): WrappedModelFnModule(\n",
       "        (_model): Linear(in_features=16, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (_node_models_model_dict): ModuleDict(\n",
       "      (tracks): WrappedModelFnModule()\n",
       "      (PVs): WrappedModelFnModule()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540c95c-473a-4fb6-a953-c8f2aa8df17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ae5f472-9cd7-42e9-b497-7df27cd0ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroNodeBlock(\n",
       "  (_received_edges_aggregator): HeteroEdgesToNodesAggregator()\n",
       "  (_sent_edges_aggregator): HeteroEdgesToNodesAggregator()\n",
       "  (_node_models_model_dict): ModuleDict(\n",
       "    (tracks): MLP(-1, 128, 128, 128, 16)\n",
       "    (PVs): MLP(-1, 128, 128, 128, 16)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._blocks[0]._node_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a36f58d2-8dea-4072-8394-bc75cb2093a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'heterognn',\n",
       " 'gnn_layers': 6,\n",
       " 'mlp_output_size': 16,\n",
       " 'mlp_layers': 4,\n",
       " 'mlp_channels': 128,\n",
       " 'weight_mlp_layers': 4,\n",
       " 'weight_mlp_channels': 16,\n",
       " 'weighted_mp': False,\n",
       " 'use_edge_weights': True,\n",
       " 'use_node_weights': True,\n",
       " 'node_types': ['tracks', 'PVs'],\n",
       " 'edge_types': ['tracks_tracks', 'tracks_PVs']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_loader.get(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1127da50-d3f9-4111-b39b-89d97ee5d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Trainer(ABC):\n",
    "\n",
    "    def __init__(self, config, model, train_loader, val_loader):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader    \n",
    "        self.train_acc = []\n",
    "        self.val_acc = [] \n",
    "        self.train_loss = []\n",
    "        self.val_loss = [] \n",
    "        self.epochs = []\n",
    "        \n",
    "    @abstractmethod\n",
    "    def eval_one_epoch(self, train=True):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def train(self, epochs=10, learning_rate=0.001):\n",
    "        pass\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "\n",
    "    def save_dataframe(self, file_name):\n",
    "        pass\n",
    "\n",
    "    def plot_loss(self, file_name=\"loss.png\", show=True):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import os\n",
    "        \n",
    "        plt.plot(self.train_loss, label=\"Train Loss\")\n",
    "        plt.plot(self.val_loss, label=\"Validation Loss\")\n",
    "        \n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('Cross Entropy Loss')\n",
    "        \n",
    "        plt.legend()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.savefig(file_name)\n",
    "        \n",
    "\n",
    "    def plot_accuracy(self, file_name=\"acc.png\", show=True):\n",
    "\n",
    "        class1_acc_vl=[]\n",
    "        class2_acc_vl=[]\n",
    "        class3_acc_vl=[]\n",
    "        class4_acc_vl=[]\n",
    "        \n",
    "        for vl_acc in self.val_acc:\n",
    "            class1_acc_vl.append(vl_acc[0])\n",
    "            class2_acc_vl.append(vl_acc[1])\n",
    "            class3_acc_vl.append(vl_acc[2])\n",
    "            class4_acc_vl.append(vl_acc[3])\n",
    "        \n",
    "        class1_acc_tr=[]\n",
    "        class2_acc_tr=[]\n",
    "        class3_acc_tr=[]\n",
    "        class4_acc_tr=[]\n",
    "        \n",
    "        for tr_acc in self.train_acc:\n",
    "            class1_acc_tr.append(tr_acc[0])\n",
    "            class2_acc_tr.append(tr_acc[1])\n",
    "            class3_acc_tr.append(tr_acc[2])\n",
    "            class4_acc_tr.append(tr_acc[3])\n",
    "            \n",
    "        fig, axarr = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                \n",
    "        axarr[0].plot(class1_acc_tr, label=\"LCA=0\")\n",
    "        axarr[0].plot(class2_acc_tr, label=\"LCA=1\")\n",
    "        axarr[0].plot(class3_acc_tr, label=\"LCA=2\")\n",
    "        axarr[0].plot(class4_acc_tr, label=\"LCA=3\")\n",
    "        \n",
    "        axarr[0].set_xlabel('epoch')\n",
    "        axarr[0].set_ylabel('training accuracy')\n",
    "        \n",
    "        axarr[0].legend()\n",
    "        \n",
    "        axarr[1].plot(class1_acc_vl, label=\"LCA=0\")\n",
    "        axarr[1].plot(class2_acc_vl, label=\"LCA=1\")\n",
    "        axarr[1].plot(class3_acc_vl, label=\"LCA=2\")\n",
    "        axarr[1].plot(class4_acc_vl, label=\"LCA=3\")\n",
    "        \n",
    "        axarr[1].set_xlabel('epoch')\n",
    "        axarr[1].set_ylabel('validation accuracy')\n",
    "        \n",
    "        axarr[1].legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if show:\n",
    "            plt.show()        \n",
    "        plt.savefig(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28ab924-3dd6-4824-b600-4e631060b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aeed9a2-5470-46ff-8a81-4247d315fbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c218fad7-e30b-4778-872b-8e52689137d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_edge_weight(loader):\n",
    "    sum_edges = 0 \n",
    "    sum_pos = 0\n",
    "    for data in loader:\n",
    "        sum_edges += data.edges.shape[0]\n",
    "        sum_pos  += torch.sum(data.y[:,0]==0).item()\n",
    "    return sum_edges/(2*sum_pos)\n",
    "\n",
    "def positive_node_weight(loader):\n",
    "    sum_nodes = 0 \n",
    "    sum_pos = 0\n",
    "    for data in loader:\n",
    "        num_nodes=data.nodes.shape[0]\n",
    "        #out = data.edges.new_zeros(num_nodes, 4)\n",
    "        node_sum = scatter_add(data.y,data.senders,dim=0)\n",
    "        ynodes = (1.*(torch.sum(node_sum[:,1:],1)>0)).unsqueeze(1)\n",
    "        sum_nodes += num_nodes\n",
    "        sum_pos  += torch.sum(ynodes==1).item()\n",
    "    return sum_nodes/(2*sum_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acba0d-4adc-456f-9830-9c55cda3f465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fde369c-ecc2-4140-a3ad-749766820629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_edge_weight(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d02d11c8-0d4d-43eb-abbf-0187d201f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_node_weight(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9706a696-260c-4066-8448-921f0b8bc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wmpgnn.util.functions import weight_four_class, acc_four_class\n",
    "\n",
    "class GNNTrainer(Trainer):\n",
    "    \"\"\" Class for training \"\"\"\n",
    "    def __init__(self, config, model, train_loader, val_loader, add_BCE=True):\n",
    "        super().__init__(config, model, train_loader, val_loader)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        weights=weight_four_class(self.train_loader)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        pos_weight = positive_edge_weight(train_loader)\n",
    "        pos_weight = torch.tensor([pos_weight])  \n",
    "        self.criterionBCE = nn.BCEWithLogitsLoss(weight=pos_weight) \n",
    "        pos_weight = positive_node_weight(train_loader)\n",
    "        pos_weight = torch.tensor([pos_weight])\n",
    "        self.criterionBCEnodes = nn.BCEWithLogitsLoss(weight=pos_weight) \n",
    "        \n",
    "        self.criterion.to('cuda')\n",
    "        self.criterionBCE.cuda()\n",
    "        self.criterionBCEnodes.cuda()\n",
    "        self.model.cuda()    \n",
    "\n",
    "        self.add_BCE = add_BCE\n",
    "        self.alpha_BCE = 0.2\n",
    "\n",
    "    def set_alpha_BCE(alpha):\n",
    "        self.alpha_BCE = alpha\n",
    "\n",
    "    def eval_one_epoch(self, train = True):\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        acc_one_epoch = []\n",
    "        if train == True:\n",
    "            data_loader = self.train_loader\n",
    "        else:\n",
    "            data_loader = self.val_loader\n",
    "        last_batch = len(data_loader)\n",
    "        for i, data in enumerate(data_loader): \n",
    "            data['graph_globals'] = data['graph_globals'].unsqueeze(1)\n",
    "            data.receivers = data.receivers - torch.min(data.receivers)\n",
    "            data.senders = data.senders - torch.min(data.senders)\n",
    "            data.edgepos = data.edgepos - torch.min(data.edgepos)\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            data.to('cuda')\n",
    "            yBCE_start = 1.*(data.y[:,0]==0).unsqueeze(1)\n",
    "            num_nodes=data.nodes.shape[0]\n",
    "            out = data.edges.new_zeros(num_nodes, data.edges.shape[1])\n",
    "            node_sum = scatter_add(data.y,data.senders,out=out,dim=0)\n",
    "            ynodes_start = (1.*(torch.sum(node_sum[:,1:],1)>0)).unsqueeze(1)\n",
    "            label0 = data.y.argmax(dim=1)\n",
    "            answers = torch.ones_like(data.edges).cuda()\n",
    "    \n",
    "            outputs = self.model(data)\n",
    "            data = outputs\n",
    "            label= data.y.argmax(dim=1)\n",
    "            num_nodes=data.nodes.shape[0]\n",
    "            out = data.edges.new_zeros(num_nodes, data.edges.shape[1])\n",
    "            node_sum = scatter_add(data.y,data.senders,out=out,dim=0)\n",
    "            ynodes = (1.*(torch.sum(node_sum[:,1:],1)>0)).unsqueeze(1)\n",
    "        \n",
    "            loss = self.criterion(outputs.edges, label) \n",
    "            yBCE = 1.*(data.y[:,0]==0).unsqueeze(1)\n",
    "            if self.add_BCE:\n",
    "                count = 0 \n",
    "                for block in self.model._blocks:\n",
    "                    loss += self.alpha_BCE*self.criterionBCE(block._network.edge_logits, yBCE)\n",
    "                    loss += self.alpha_BCE*self.criterionBCEnodes(block._network.node_logits, ynodes)\n",
    "                    count += 1\n",
    "        \n",
    "            \n",
    "            acc_one_batch = acc_four_class(outputs.edges, label)\n",
    "            acc_one_epoch.append(acc_one_batch)\n",
    "            if train:\n",
    "                loss.backward()            \n",
    "                self.optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            if (i+1) == last_batch:\n",
    "                last_loss = running_loss / last_batch # loss per batch\n",
    "                print('  batch {} last_batch {} loss: {}'.format(i + 1, last_batch, last_loss))\n",
    "    \n",
    "                running_loss = 0.\n",
    "        \n",
    "        acc_one_epoch=torch.stack(acc_one_epoch)\n",
    "    \n",
    "        return last_loss, acc_one_epoch.nanmean(dim=0)        \n",
    "        \n",
    "    def train(self, epochs=10, starting_epoch =0, learning_rate=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(starting_epoch, epochs):\n",
    "            print(f\"At epoch {epoch}\")\n",
    "            self.epochs.append(epoch)\n",
    "            train_loss, train_acc = self.eval_one_epoch()\n",
    "            self.model.train(False)\n",
    "            val_loss, val_acc = self.eval_one_epoch(train=False)    \n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            print(f'Train Loss: {train_loss:03f}')\n",
    "            print(f'Val Loss: {val_loss:03f}')\n",
    "            print(f'Train Acc: {train_acc}')\n",
    "            print(f'Val Acc: {val_acc}')\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30191db1-dbdf-4d4b-aa03-35b5a0c6e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00d2d0de-d9bc-4e12-9aad-e3cd837cdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wmpgnn.util.functions import weight_four_class, acc_four_class\n",
    "\n",
    "\n",
    "def hetero_positive_edge_weight(loader):\n",
    "    sum_edges = 0 \n",
    "    sum_pos = 0\n",
    "    for data in loader:\n",
    "        sum_edges += data[('tracks','to','tracks')].edges.shape[0]\n",
    "        sum_pos  += torch.sum(data[('tracks','to','tracks')].y[:,0]==0).item()\n",
    "    return sum_edges/(2*sum_pos)\n",
    "\n",
    "def hetero_positive_node_weight(loader):\n",
    "    sum_nodes = 0 \n",
    "    sum_pos = 0\n",
    "    for data in loader:\n",
    "        num_nodes=data['tracks'].x.shape[0]\n",
    "        #out = data.edges.new_zeros(num_nodes, 4)\n",
    "        node_sum = scatter_add(data[('tracks','to','tracks')].y, data[('tracks','to','tracks')].edge_index[0],dim=0)\n",
    "        ynodes = (1.*(torch.sum(node_sum[:,1:],1)>0)).unsqueeze(1)\n",
    "        sum_nodes += num_nodes\n",
    "        sum_pos  += torch.sum(ynodes==1).item()\n",
    "    return sum_nodes/(2*sum_pos)\n",
    "    \n",
    "class HeteroGNNTrainer(Trainer):\n",
    "    \"\"\" Class for training \"\"\"\n",
    "    def __init__(self, config, model, train_loader, val_loader, add_BCE=True):\n",
    "        super().__init__(config, model, train_loader, val_loader)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        weights=weight_four_class(self.train_loader,hetero=True)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        pos_weight = hetero_positive_edge_weight(train_loader)\n",
    "        pos_weight = torch.tensor([pos_weight])  \n",
    "        print(pos_weight)\n",
    "        #self.criterionBCE = nn.BCELoss(weight=pos_weight) \n",
    "        self.criterionBCE = nn.BCEWithLogitsLoss(pos_weight=pos_weight) \n",
    "        pos_weight = hetero_positive_node_weight(train_loader)\n",
    "        pos_weight = torch.tensor([pos_weight])\n",
    "        print(pos_weight)\n",
    "        #self.criterionBCEnodes = nn.BCELoss(weight=pos_weight) \n",
    "        self.criterionBCEnodes = nn.BCEWithLogitsLoss(pos_weight=pos_weight) \n",
    "        pos_weight = torch.tensor([6.1118])\n",
    "        #self.criterionBCE_PV = nn.BCELoss(weight=pos_weight) \n",
    "        self.criterionBCE_PV = nn.BCEWithLogitsLoss(pos_weight=pos_weight) \n",
    "        self.criterionBCE_PV.cuda()\n",
    "        \n",
    "        self.criterion.to('cuda')\n",
    "        self.criterionBCE.cuda()\n",
    "        self.criterionBCEnodes.cuda()\n",
    "        self.model.cuda()    \n",
    "\n",
    "        self.add_BCE = add_BCE\n",
    "        self.alpha_BCE = 0.2\n",
    "\n",
    "        self.train_PV_acc = []\n",
    "        self.val_PV_acc = []\n",
    "\n",
    "    def set_alpha_BCE(alpha):\n",
    "        self.alpha_BCE = alpha\n",
    "        \n",
    "    def eval_one_epoch(self, train = True):\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        acc_one_epoch = []\n",
    "        PV_acc_one_epoch = []\n",
    "        PV_node_acc_one_epoch = []\n",
    "        if train == True:\n",
    "            data_loader = self.train_loader\n",
    "        else:\n",
    "            data_loader = self.val_loader\n",
    "        last_batch = len(data_loader)\n",
    "        #print(last_batch)\n",
    "        for i, data in enumerate(data_loader): \n",
    "            #print(i, train)\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "            data.to('cuda')\n",
    "            data['tracks'].x = torch.hstack( [ data['tracks'].x[:,:6] , data['tracks'].x[:,9:10] ] )\n",
    "            \n",
    "            outputs = self.model(data)\n",
    "\n",
    "            data = outputs\n",
    "            \n",
    "            label= data[('tracks', 'to', 'tracks')].y.argmax(dim=1)\n",
    "            PVlabel= torch.tensor(data[('tracks', 'to', 'PVs')].y,dtype=torch.float32)\n",
    "\n",
    "            loss = self.criterion(outputs[('tracks', 'to', 'tracks')].edges, label) \n",
    "\n",
    "            num_nodes=data['tracks'].x.shape[0]\n",
    "            out = data[('tracks', 'to', 'tracks')].edges.new_zeros(num_nodes, data[('tracks', 'to', 'tracks')].y.shape[1])\n",
    "            node_sum = scatter_add(data[('tracks', 'to', 'tracks')].y,data[('tracks', 'to', 'tracks')].edge_index[0],out=out,dim=0)\n",
    "            ynodes = (1.*(torch.sum(node_sum[:,1:],1)>0)).unsqueeze(1)\n",
    "            yBCE = 1.*(data[('tracks', 'to', 'tracks')].y[:,0]==0).unsqueeze(1)\n",
    "    \n",
    "            yb = ynodes[data[('tracks', 'to', 'PVs')]['edge_index'][0]]*data[('tracks', 'to', 'PVs')].y\n",
    "            pv_sum = scatter_add(yb,data[('tracks', 'to', 'PVs')].edge_index[1],dim=0)\n",
    "            pv_target = 1.*(pv_sum > 0)\n",
    "            \n",
    "            if self.add_BCE:\n",
    "                for block in self.model._blocks:\n",
    "                    # loss += 1*self.criterionBCE(block.edge_weights[('tracks', 'to', 'tracks')], yBCE)\n",
    "                    # loss += 1*self.criterionBCE_PV(block.edge_weights[('tracks', 'to', 'PVs')], PVlabel)  \n",
    "                    # loss += 1*self.criterionBCEnodes(block.node_weights['tracks'], ynodes)\n",
    "                    # loss += 1*self.criterionBCEnodes(block.node_weights['tracks'], ynodes)\n",
    "                    loss += 1*self.criterionBCE(block.edge_logits[('tracks', 'to', 'tracks')], yBCE)\n",
    "                    loss += 1*self.criterionBCE_PV(block.edge_logits[('tracks', 'to', 'PVs')], PVlabel)  \n",
    "                    loss += 1*self.criterionBCEnodes(block.node_logits['tracks'], ynodes)\n",
    "                    #loss += 1*self.criterionBCEnodes(block.node_logits['PVs'], pv_target)\n",
    "            acc_one_batch = acc_four_class(outputs[('tracks', 'to', 'tracks')].edges, label)\n",
    "            acc_one_epoch.append(acc_one_batch)\n",
    "\n",
    "            PV_acc_one_batch =  torch.sum(PVlabel == ( self.model._blocks[-1].edge_weights[('tracks', 'to', 'PVs')]> 0.5 ))/ PVlabel.shape[0]\n",
    "            PV_acc_one_epoch.append(PV_acc_one_batch)     \n",
    "\n",
    "            PV_node_acc_one_batch =  torch.sum(pv_target == ( self.model._blocks[-1].node_weights['PVs']> 0.5 ))/ pv_target.shape[0]\n",
    "            PV_node_acc_one_epoch.append(PV_node_acc_one_batch)    \n",
    "            if train:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            if (i+1) == last_batch:\n",
    "                last_loss = running_loss / last_batch # loss per batch\n",
    "                print('  batch {} last_batch {} loss: {}'.format(i + 1, last_batch, last_loss))\n",
    "    \n",
    "                running_loss = 0.\n",
    "        \n",
    "        acc_one_epoch=torch.stack(acc_one_epoch)\n",
    "        PV_acc_one_epoch=torch.stack(PV_acc_one_epoch)\n",
    "        PV_node_acc_one_epoch=torch.stack(PV_node_acc_one_epoch)\n",
    "\n",
    "        return last_loss, acc_one_epoch.nanmean(dim=0), PV_acc_one_epoch.nanmean(dim=0), PV_node_acc_one_epoch.nanmean(dim=0)\n",
    "            \n",
    "     \n",
    "    def train(self, epochs=10, starting_epoch =0, learning_rate=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(starting_epoch, epochs):\n",
    "            print(f\"At epoch {epoch}\")\n",
    "            self.epochs.append(epoch)\n",
    "            train_loss, train_acc, train_PV_acc, train_B_PV_acc = self.eval_one_epoch()\n",
    "            self.model.train(False)\n",
    "            val_loss, val_acc, val_PV_acc, val_B_PV_acc = self.eval_one_epoch(train=False)    \n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            self.train_PV_acc.append(train_PV_acc)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            self.val_PV_acc.append(val_PV_acc)\n",
    "            print(f'Train Loss: {train_loss:03f}')\n",
    "            print(f'Val Loss: {val_loss:03f}')\n",
    "            print(f'Train Acc: {train_acc}')\n",
    "            print(f'Val Acc: {val_acc}')\n",
    "            print(f'Train PV edge Acc: {train_PV_acc}')\n",
    "            print(f'Val PV edge Acc: {val_PV_acc}')\n",
    "            print(f'Train PV edge Acc: {train_B_PV_acc}')\n",
    "            print(f'Val PV edge Acc: {val_B_PV_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddd29a91-d3d8-4689-85b3-f31314d40561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_loader = ModelLoader(config_loader)\n",
    "#model = model_loader.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289ac4a-e8b6-479f-934c-f3ae9dc0a2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7142a061-749d-4b22-a8fb-9bfdc04beef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8840bfc3-20eb-4747-9904-ba37445a0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = torch.tensor([2.5158e-01, 1.7982e+02, 6.0619e+01, 3.2675e+02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86e077e0-481a-4ee0-aac4-6034ca3eaab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5165e-01, 1.7152e+02, 5.7953e+01, 3.1798e+02])\n",
      "tensor([76.2227])\n",
      "tensor([8.5526])\n"
     ]
    }
   ],
   "source": [
    "trainer  = HeteroGNNTrainer(config_loader, model, train_loader, val_loader, add_BCE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b0f2ece-724c-4f9f-b855-afe8bd3bff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507829/4191225867.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  PVlabel= torch.tensor(data[('tracks', 'to', 'PVs')].y,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1234 last_batch 1234 loss: 2.8590355613050042\n",
      "  batch 290 last_batch 290 loss: 2.8858507131708078\n",
      "Train Loss: 2.859036\n",
      "Val Loss: 2.885851\n",
      "Train Acc: tensor([0.9763, 0.6959, 0.5021, 0.8447])\n",
      "Val Acc: tensor([0.9817, 0.7637, 0.4233, 0.8571])\n",
      "Train PV edge Acc: 0.9925808310508728\n",
      "Val PV edge Acc: 0.9917919039726257\n",
      "Train PV edge Acc: 0.9791470766067505\n",
      "Val PV edge Acc: 0.9788030982017517\n",
      "At epoch 1\n",
      "  batch 1234 last_batch 1234 loss: 2.809573397740746\n",
      "  batch 290 last_batch 290 loss: 2.9316425019297108\n",
      "Train Loss: 2.809573\n",
      "Val Loss: 2.931643\n",
      "Train Acc: tensor([0.9773, 0.7004, 0.5084, 0.8475])\n",
      "Val Acc: tensor([0.9836, 0.7363, 0.4767, 0.8367])\n",
      "Train PV edge Acc: 0.9928807616233826\n",
      "Val PV edge Acc: 0.992691159248352\n",
      "Train PV edge Acc: 0.9814252257347107\n",
      "Val PV edge Acc: 0.9837502241134644\n",
      "At epoch 2\n",
      "  batch 1234 last_batch 1234 loss: 2.7549289445629768\n",
      "  batch 290 last_batch 290 loss: 2.987028119481843\n",
      "Train Loss: 2.754929\n",
      "Val Loss: 2.987028\n",
      "Train Acc: tensor([0.9775, 0.7023, 0.5130, 0.8507])\n",
      "Val Acc: tensor([0.9835, 0.6705, 0.5330, 0.8342])\n",
      "Train PV edge Acc: 0.9932696223258972\n",
      "Val PV edge Acc: 0.9928324818611145\n",
      "Train PV edge Acc: 0.9832310676574707\n",
      "Val PV edge Acc: 0.9836293458938599\n",
      "At epoch 3\n",
      "  batch 1234 last_batch 1234 loss: 2.7872931002797725\n",
      "  batch 290 last_batch 290 loss: 2.8937126102118658\n",
      "Train Loss: 2.787293\n",
      "Val Loss: 2.893713\n",
      "Train Acc: tensor([0.9774, 0.7024, 0.5143, 0.8532])\n",
      "Val Acc: tensor([0.9849, 0.7378, 0.4843, 0.8284])\n",
      "Train PV edge Acc: 0.9930267333984375\n",
      "Val PV edge Acc: 0.9923927187919617\n",
      "Train PV edge Acc: 0.9830480813980103\n",
      "Val PV edge Acc: 0.9833675026893616\n",
      "At epoch 4\n",
      "  batch 1234 last_batch 1234 loss: 2.7312479743694755\n",
      "  batch 290 last_batch 290 loss: 3.1153759931695872\n",
      "Train Loss: 2.731248\n",
      "Val Loss: 3.115376\n",
      "Train Acc: tensor([0.9782, 0.7048, 0.5190, 0.8553])\n",
      "Val Acc: tensor([0.9836, 0.6775, 0.5129, 0.8228])\n",
      "Train PV edge Acc: 0.9933092594146729\n",
      "Val PV edge Acc: 0.9912353754043579\n",
      "Train PV edge Acc: 0.9834864735603333\n",
      "Val PV edge Acc: 0.9837785363197327\n",
      "At epoch 5\n",
      "  batch 1234 last_batch 1234 loss: 2.701904645325492\n",
      "  batch 290 last_batch 290 loss: 2.8245543586796726\n",
      "Train Loss: 2.701905\n",
      "Val Loss: 2.824554\n",
      "Train Acc: tensor([0.9782, 0.7077, 0.5199, 0.8526])\n",
      "Val Acc: tensor([0.9831, 0.7496, 0.4732, 0.8529])\n",
      "Train PV edge Acc: 0.9932851195335388\n",
      "Val PV edge Acc: 0.9925745725631714\n",
      "Train PV edge Acc: 0.9842846989631653\n",
      "Val PV edge Acc: 0.9843854308128357\n",
      "At epoch 6\n",
      "  batch 1234 last_batch 1234 loss: 2.657141800637763\n",
      "  batch 290 last_batch 290 loss: 2.8550873337120843\n",
      "Train Loss: 2.657142\n",
      "Val Loss: 2.855087\n",
      "Train Acc: tensor([0.9786, 0.7090, 0.5245, 0.8592])\n",
      "Val Acc: tensor([0.9870, 0.7740, 0.4444, 0.8368])\n",
      "Train PV edge Acc: 0.9935160279273987\n",
      "Val PV edge Acc: 0.993021547794342\n",
      "Train PV edge Acc: 0.9823694229125977\n",
      "Val PV edge Acc: 0.9851905703544617\n",
      "At epoch 7\n",
      "  batch 1234 last_batch 1234 loss: 2.6532685878404343\n",
      "  batch 290 last_batch 290 loss: 2.7986886690402852\n",
      "Train Loss: 2.653269\n",
      "Val Loss: 2.798689\n",
      "Train Acc: tensor([0.9784, 0.7063, 0.5307, 0.8554])\n",
      "Val Acc: tensor([0.9822, 0.7291, 0.5163, 0.8339])\n",
      "Train PV edge Acc: 0.993510901927948\n",
      "Val PV edge Acc: 0.993773341178894\n",
      "Train PV edge Acc: 0.9838381409645081\n",
      "Val PV edge Acc: 0.9849783778190613\n",
      "At epoch 8\n",
      "  batch 1234 last_batch 1234 loss: 2.6150821426119566\n",
      "  batch 290 last_batch 290 loss: 2.789644402471082\n",
      "Train Loss: 2.615082\n",
      "Val Loss: 2.789644\n",
      "Train Acc: tensor([0.9787, 0.7167, 0.5311, 0.8575])\n",
      "Val Acc: tensor([0.9846, 0.7675, 0.4812, 0.8086])\n",
      "Train PV edge Acc: 0.9937283992767334\n",
      "Val PV edge Acc: 0.9930973649024963\n",
      "Train PV edge Acc: 0.9833540320396423\n",
      "Val PV edge Acc: 0.982898473739624\n",
      "At epoch 9\n",
      "  batch 1234 last_batch 1234 loss: 2.889103358529955\n",
      "  batch 290 last_batch 290 loss: 3.874445278891202\n",
      "Train Loss: 2.889103\n",
      "Val Loss: 3.874445\n",
      "Train Acc: tensor([0.9760, 0.7075, 0.5181, 0.8367])\n",
      "Val Acc: tensor([0.9701, 0.6580, 0.3822, 0.8404])\n",
      "Train PV edge Acc: 0.9916456937789917\n",
      "Val PV edge Acc: 0.9838287234306335\n",
      "Train PV edge Acc: 0.9797711372375488\n",
      "Val PV edge Acc: 0.968975841999054\n"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "963f11f6-27f2-46ba-99d3-42a9008f34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507829/4191225867.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  PVlabel= torch.tensor(data[('tracks', 'to', 'PVs')].y,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1234 last_batch 1234 loss: 3.289735750209184\n",
      "  batch 290 last_batch 290 loss: 3.0422112662216714\n",
      "Train Loss: 3.289736\n",
      "Val Loss: 3.042211\n",
      "Train Acc: tensor([0.9733, 0.6762, 0.5066, 0.8298])\n",
      "Val Acc: tensor([0.9737, 0.7502, 0.4813, 0.8409])\n",
      "Train PV edge Acc: 0.9886472821235657\n",
      "Val PV edge Acc: 0.9903491735458374\n",
      "Train PV edge Acc: 0.978159487247467\n",
      "Val PV edge Acc: 0.978970468044281\n",
      "At epoch 11\n",
      "  batch 1234 last_batch 1234 loss: 2.828364562370016\n",
      "  batch 290 last_batch 290 loss: 2.7923335231583692\n",
      "Train Loss: 2.828365\n",
      "Val Loss: 2.792334\n",
      "Train Acc: tensor([0.9780, 0.7158, 0.5333, 0.8540])\n",
      "Val Acc: tensor([0.9767, 0.7547, 0.5050, 0.8484])\n",
      "Train PV edge Acc: 0.9919545650482178\n",
      "Val PV edge Acc: 0.9925172924995422\n",
      "Train PV edge Acc: 0.9785208702087402\n",
      "Val PV edge Acc: 0.9764626622200012\n"
     ]
    }
   ],
   "source": [
    "trainer.train(12, starting_epoch=10,learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc17e8-e5e2-4abd-9d89-57b9f18d2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(32, starting_epoch=30,learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69b49b-507b-4b95-9f52-8b4a56ae5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_loss(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d644e19-a4c5-4466-94d1-4607d1a827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91577ef9-6655-4640-a206-0a206370a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75160976-b548-4418-8382-099b9dfb61bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3532f-eedf-4291-afe9-e8f3d38d1b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
